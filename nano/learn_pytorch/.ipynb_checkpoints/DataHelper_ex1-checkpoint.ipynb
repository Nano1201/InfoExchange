{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Vocabulary(object):  #vocabulary set\n",
    "\n",
    "    def __init__(self):\n",
    "        self.char2idx = {'SOS': 0, 'EOS': 1, 'PAD': 2, 'UNK': 3}\n",
    "        self.idx2char = {0: 'SOS', 1: 'EOS', 2: 'PAD', 3: 'UNK'}\n",
    "        self.num_chars = 4  # num of char\n",
    "        self.max_length = 0   #max length word\n",
    "        self.word_list = []   #['word1','word2','word3',....]\n",
    "\n",
    "    def build_vocab(self, data_path):\n",
    "        \"\"\"Construct the relation between words and indices\"\"\"\n",
    "        with open(data_path, 'r', encoding='utf-8') as dataset:\n",
    "            for word in dataset:\n",
    "                word = word.strip('\\n')\n",
    "\n",
    "                self.word_list.append(word)      # renew self.max_length\n",
    "                if self.max_length < len(word):\n",
    "                    self.max_length = len(word)\n",
    "\n",
    "                chars = self.split_sequence(word)\n",
    "                for char in chars:\n",
    "                    if char not in self.char2idx:\n",
    "                        self.char2idx[char] = self.num_chars\n",
    "                        self.idx2char[self.num_chars] = char\n",
    "                        self.num_chars += 1\n",
    "\n",
    "    def sequence_to_indices(self, sequence, add_eos=False, add_sos=False):  #reture a list ex: [28,22,13,1,2]\n",
    "        \"\"\"Transform a char sequence to index sequence\n",
    "            :param sequence: a string composed with chars\n",
    "            :param add_eos: if true, add the <EOS> tag at the end of given sentence\n",
    "            :param add_sos: if true, add the <SOS> tag at the beginning of given sentence\n",
    "        \"\"\"\n",
    "        index_sequence = [self.char2idx['SOS']] if add_sos else []\n",
    "\n",
    "        for char in self.split_sequence(sequence):\n",
    "            if char not in self.char2idx:\n",
    "                index_sequence.append((self.char2idx['UNK']))\n",
    "            else:\n",
    "                index_sequence.append(self.char2idx[char])\n",
    "\n",
    "        if add_eos:\n",
    "            index_sequence.append(self.char2idx['EOS'])\n",
    "\n",
    "        return index_sequence\n",
    "\n",
    "    def indices_to_sequence(self, indices):  #return string 'apple'\n",
    "        \"\"\"Transform a list of indices\n",
    "            :param indices: a list\n",
    "        \"\"\"\n",
    "        sequence = \"\"\n",
    "        for idx in indices:\n",
    "            char = self.idx2char[idx]\n",
    "            if char == \"EOS\":\n",
    "                break\n",
    "            else:\n",
    "                sequence += char\n",
    "        return sequence\n",
    "        \"\"\"\n",
    "        sequence = \"\"\n",
    "        for idx in indices:\n",
    "            if idx.item() in self.idx2char:\n",
    "                char = self.idx2char[idx.item()]\n",
    "                if char == \"EOS\":\n",
    "                    break\n",
    "                else:\n",
    "                    sequence += char\n",
    "            else:\n",
    "                print(idx)\n",
    "        return sequence\n",
    "        \"\"\"\n",
    "    def split_sequence(self, sequence):   #Return: [a, l, p, h, a, b, e, t]\n",
    "        \"\"\"Vary from languages and tasks. In our task, we simply return chars in given sentence\n",
    "        For example:\n",
    "            Input : alphabet\n",
    "            Return: [a, l, p, h, a, b, e, t]\n",
    "        \"\"\"\n",
    "        return [char for char in sequence]\n",
    "\n",
    "    def __str__(self):\n",
    "        str = \"Vocab information:\\n\"\n",
    "        for idx, char in self.idx2char.items():\n",
    "            str += \"Char: %s Index: %d\\n\" % (char, idx)\n",
    "        return str\n",
    "\n",
    "\n",
    "class DataTransformer(object):\n",
    "\n",
    "    def __init__(self, path, use_cuda):\n",
    "        self.indices_sequences = []\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # Load and build the vocab\n",
    "        self.vocab = Vocabulary()\n",
    "        self.vocab.build_vocab(path)\n",
    "        self.PAD_ID = self.vocab.char2idx[\"PAD\"]\n",
    "        self.SOS_ID = self.vocab.char2idx[\"SOS\"]\n",
    "        self.vocab_size = self.vocab.num_chars\n",
    "        self.max_length = self.vocab.max_length\n",
    "\n",
    "        self._build_training_set(path)\n",
    "\n",
    "    def _build_training_set(self, path):   #prepare indices_sequences from vocan.word_list\n",
    "        # Change sentences to indices, and append <EOS> at the end of all pairs\n",
    "        for word in self.vocab.word_list:\n",
    "            indices_seq = self.vocab.sequence_to_indices(word, add_eos=True)\n",
    "            # input and target are the same in auto-encoder\n",
    "            self.indices_sequences.append([indices_seq, indices_seq[:]])   #same  ex:[[4, 5, 6, 1], [4, 5, 6, 1]]\n",
    "\n",
    "    def mini_batches(self, batch_size):\n",
    "        input_batches = []  #useless\n",
    "        target_batches = []  #useless\n",
    "\n",
    "        np.random.shuffle(self.indices_sequences)\n",
    "        mini_batches = [                            # if batch_size = 3 : [ [1st pair,2nd pari,3nd pair],[4..,5..,6..],[7,8,9],[10,11,12].... ]\n",
    "            self.indices_sequences[k: k + batch_size]\n",
    "            for k in range(0, len(self.indices_sequences), batch_size)\n",
    "        ]\n",
    "\n",
    "        for batch in mini_batches:\n",
    "            seq_pairs = sorted(batch, key=lambda seqs: len(seqs[0]), reverse=True)  # sorted by input_lengths\n",
    "            input_seqs = [pair[0] for pair in seq_pairs]\n",
    "            target_seqs = [pair[1] for pair in seq_pairs]\n",
    "\n",
    "            input_lengths = [len(s) for s in input_seqs]\n",
    "            in_max = input_lengths[0]  #already sorted, so [0] is longest\n",
    "            input_padded = [self.pad_sequence(s, in_max) for s in input_seqs]\n",
    "\n",
    "            target_lengths = [len(s) for s in target_seqs]\n",
    "            out_max = target_lengths[0]\n",
    "            target_padded = [self.pad_sequence(s, out_max) for s in target_seqs]\n",
    "\n",
    "            #input&targer to tensor variable\n",
    "            input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)  \n",
    "            target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)  # time * batch\n",
    "\n",
    "            if self.use_cuda:\n",
    "                input_var = input_var.cuda()\n",
    "                target_var = target_var.cuda()\n",
    "\n",
    "            yield (input_var, input_lengths), (target_var, target_lengths)   # ( [7,8,9,3,1],5),([7,8,9,3,1],5)\n",
    "\n",
    "    def pad_sequence(self, sequence, max_length):\n",
    "        sequence += [self.PAD_ID for i in range(max_length - len(sequence))]\n",
    "        return sequence\n",
    "\n",
    "    def evaluation_batch(self, words):\n",
    "        \"\"\"\n",
    "        Prepare a batch of var for evaluating\n",
    "        :param words: a list, store the testing data \n",
    "        :return: evaluation_batch\n",
    "        \"\"\"\n",
    "        evaluation_batch = []\n",
    "\n",
    "        for word in words:\n",
    "            indices_seq = self.vocab.sequence_to_indices(word, add_eos=True)\n",
    "            evaluation_batch.append([indices_seq])\n",
    "\n",
    "        seq_pairs = sorted(evaluation_batch, key=lambda seqs: len(seqs[0]), reverse=True)\n",
    "        input_seqs = [pair[0] for pair in seq_pairs]\n",
    "        input_lengths = [len(s) for s in input_seqs]\n",
    "        in_max = input_lengths[0]\n",
    "        input_padded = [self.pad_sequence(s, in_max) for s in input_seqs]\n",
    "\n",
    "        input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)  # time * batch\n",
    "\n",
    "        if self.use_cuda:\n",
    "            input_var = input_var.cuda()\n",
    "\n",
    "        return input_var, input_lengths\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    vocab = Vocabulary()\n",
    "    vocab.build_vocab('Google-10000-English.txt')\n",
    "    print(vocab)\n",
    "\n",
    "    test = \"helloworld\"\n",
    "    print(\"Sequence before transformed:\", test)\n",
    "    ids = vocab.sequence_to_indices(test)\n",
    "    print(\"Indices sequence:\", ids)\n",
    "    sent = vocab.indices_to_sequence(ids)\n",
    "    print(\"Sequence after transformed:\",sent,\"\\n\")\n",
    "\n",
    "    data_transformer = DataTransformer('Google-10000-English.txt', use_cuda=False)\n",
    "\n",
    "    for ib, tb in data_transformer.mini_batches(batch_size=3):\n",
    "        print(\"\\n\\nB0-0\")\n",
    "        print(ib)#,'\\n\\n', tb)\n",
    "        \n",
    "        print('------------')\n",
    "        embedding = nn.Embedding(30,3)\n",
    "        input=ib[0]\n",
    "        embedded=embedding(input)\n",
    "        print(embedded)\n",
    "        \n",
    "        print('============================================')\n",
    "        leng = ib[1]\n",
    "        print(type(leng))\n",
    "        print(leng)\n",
    "        packed = pack_padded_sequence(embedded, leng)\n",
    "        print('\\n\\n')\n",
    "        print(packed)\n",
    "        break\n",
    "\"\"\"\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show vocab dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab information:\n",
      "Char: SOS Index: 0\n",
      "Char: EOS Index: 1\n",
      "Char: PAD Index: 2\n",
      "Char: UNK Index: 3\n",
      "Char: t Index: 4\n",
      "Char: h Index: 5\n",
      "Char: e Index: 6\n",
      "Char: o Index: 7\n",
      "Char: f Index: 8\n",
      "Char: a Index: 9\n",
      "Char: n Index: 10\n",
      "Char: d Index: 11\n",
      "Char: i Index: 12\n",
      "Char: r Index: 13\n",
      "Char: s Index: 14\n",
      "Char: b Index: 15\n",
      "Char: y Index: 16\n",
      "Char: w Index: 17\n",
      "Char: u Index: 18\n",
      "Char: m Index: 19\n",
      "Char: l Index: 20\n",
      "Char: v Index: 21\n",
      "Char: c Index: 22\n",
      "Char: p Index: 23\n",
      "Char: g Index: 24\n",
      "Char: k Index: 25\n",
      "Char: x Index: 26\n",
      "Char: j Index: 27\n",
      "Char: z Index: 28\n",
      "Char: q Index: 29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.build_vocab('Google-10000-English.txt')\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sequence to indices example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence before transformed: helloworld\n",
      "Indices sequence: [5, 6, 20, 20, 7, 17, 7, 13, 20, 11]\n",
      "Sequence after transformed: helloworld \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = \"helloworld\"\n",
    "print(\"Sequence before transformed:\", test)\n",
    "ids = vocab.sequence_to_indices(test)\n",
    "print(\"Indices sequence:\", ids)\n",
    "sent = vocab.indices_to_sequence(ids)\n",
    "print(\"Sequence after transformed:\",sent,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding & pack_padding_sequence example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1st batch:\n",
      "input:\n",
      " (tensor([[11,  9, 18],\n",
      "        [12, 20, 10],\n",
      "        [14, 22, 12],\n",
      "        [23,  7,  4],\n",
      "        [20,  5,  1],\n",
      "        [ 9,  7,  2],\n",
      "        [16, 20,  2],\n",
      "        [ 6,  1,  2],\n",
      "        [11,  2,  2],\n",
      "        [ 1,  2,  2]]), [10, 8, 5]) \n",
      "\n",
      " target:\n",
      " (tensor([[11,  9, 18],\n",
      "        [12, 20, 10],\n",
      "        [14, 22, 12],\n",
      "        [23,  7,  4],\n",
      "        [20,  5,  1],\n",
      "        [ 9,  7,  2],\n",
      "        [16, 20,  2],\n",
      "        [ 6,  1,  2],\n",
      "        [11,  2,  2],\n",
      "        [ 1,  2,  2]]), [10, 8, 5])\n",
      "\n",
      "------------\n",
      "after embedding(30,3):\n",
      "\n",
      "tensor([[[-0.3962, -0.3535, -2.0223],\n",
      "         [ 1.4845,  1.2232,  0.3405],\n",
      "         [ 1.2795,  0.8027, -1.3001]],\n",
      "\n",
      "        [[-0.5129,  0.2063,  1.1299],\n",
      "         [ 0.2351,  0.2509,  0.5732],\n",
      "         [ 0.6289, -0.7532, -0.7337]],\n",
      "\n",
      "        [[ 1.3063,  0.3306, -1.2231],\n",
      "         [ 1.6357,  0.7090, -0.0940],\n",
      "         [-0.5129,  0.2063,  1.1299]],\n",
      "\n",
      "        [[-0.8622, -0.8680, -0.4603],\n",
      "         [ 0.3462,  0.1498,  1.1750],\n",
      "         [ 1.6731,  0.6317, -1.0507]],\n",
      "\n",
      "        [[ 0.2351,  0.2509,  0.5732],\n",
      "         [-0.1719, -0.2810,  0.9664],\n",
      "         [ 0.4218,  0.3962, -0.5799]],\n",
      "\n",
      "        [[ 1.4845,  1.2232,  0.3405],\n",
      "         [ 0.3462,  0.1498,  1.1750],\n",
      "         [-0.8240,  0.4166, -0.3146]],\n",
      "\n",
      "        [[-0.4780,  0.4295,  0.8186],\n",
      "         [ 0.2351,  0.2509,  0.5732],\n",
      "         [-0.8240,  0.4166, -0.3146]],\n",
      "\n",
      "        [[ 1.6598,  0.4541,  0.8160],\n",
      "         [ 0.4218,  0.3962, -0.5799],\n",
      "         [-0.8240,  0.4166, -0.3146]],\n",
      "\n",
      "        [[-0.3962, -0.3535, -2.0223],\n",
      "         [-0.8240,  0.4166, -0.3146],\n",
      "         [-0.8240,  0.4166, -0.3146]],\n",
      "\n",
      "        [[ 0.4218,  0.3962, -0.5799],\n",
      "         [-0.8240,  0.4166, -0.3146],\n",
      "         [-0.8240,  0.4166, -0.3146]]], grad_fn=<EmbeddingBackward>)\n",
      "============================================\n",
      "<class 'list'>\n",
      "[10, 8, 5]\n",
      "\n",
      "\n",
      "\n",
      "PackedSequence(data=tensor([[-0.3962, -0.3535, -2.0223],\n",
      "        [ 1.4845,  1.2232,  0.3405],\n",
      "        [ 1.2795,  0.8027, -1.3001],\n",
      "        [-0.5129,  0.2063,  1.1299],\n",
      "        [ 0.2351,  0.2509,  0.5732],\n",
      "        [ 0.6289, -0.7532, -0.7337],\n",
      "        [ 1.3063,  0.3306, -1.2231],\n",
      "        [ 1.6357,  0.7090, -0.0940],\n",
      "        [-0.5129,  0.2063,  1.1299],\n",
      "        [-0.8622, -0.8680, -0.4603],\n",
      "        [ 0.3462,  0.1498,  1.1750],\n",
      "        [ 1.6731,  0.6317, -1.0507],\n",
      "        [ 0.2351,  0.2509,  0.5732],\n",
      "        [-0.1719, -0.2810,  0.9664],\n",
      "        [ 0.4218,  0.3962, -0.5799],\n",
      "        [ 1.4845,  1.2232,  0.3405],\n",
      "        [ 0.3462,  0.1498,  1.1750],\n",
      "        [-0.4780,  0.4295,  0.8186],\n",
      "        [ 0.2351,  0.2509,  0.5732],\n",
      "        [ 1.6598,  0.4541,  0.8160],\n",
      "        [ 0.4218,  0.3962, -0.5799],\n",
      "        [-0.3962, -0.3535, -2.0223],\n",
      "        [ 0.4218,  0.3962, -0.5799]], grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([3, 3, 3, 3, 3, 2, 2, 2, 1, 1]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    }
   ],
   "source": [
    "data_transformer = DataTransformer('Google-10000-English.txt', use_cuda=False)\n",
    "\n",
    "for ib, tb in data_transformer.mini_batches(batch_size=3):\n",
    "    print(\"\\n1st batch:\")\n",
    "    print('input:\\n',ib,'\\n\\n','target:\\n', tb)\n",
    "        \n",
    "    print('\\n------------\\nafter embedding(30,3):\\n')\n",
    "    embedding = nn.Embedding(30,3)\n",
    "    input=ib[0]\n",
    "    embedded=embedding(input)\n",
    "    print(embedded)\n",
    "        \n",
    "    print('============================================')\n",
    "    leng = ib[1]\n",
    "    #print(type(leng))\n",
    "    #print(leng)\n",
    "    packed = pack_padded_sequence(embedded, leng)\n",
    "    print('\\n\\n')\n",
    "    print(packed)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding & pack_padding_sequence example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.4367, -1.2376, -0.7855],\n",
      "         [ 0.1500,  0.1474, -0.2132],\n",
      "         [ 1.4367, -1.2376, -0.7855],\n",
      "         [ 1.1864,  0.5637, -1.1194],\n",
      "         [ 0.3134,  0.5307, -0.2227]],\n",
      "\n",
      "        [[ 0.8141, -0.1185, -0.0485],\n",
      "         [ 1.1864,  0.5637, -1.1194],\n",
      "         [ 0.3134,  0.5307, -0.2227],\n",
      "         [ 0.3134,  0.5307, -0.2227],\n",
      "         [ 0.3134,  0.5307, -0.2227]],\n",
      "\n",
      "        [[-0.0411,  0.3431,  0.2645],\n",
      "         [-0.0411,  0.3431,  0.2645],\n",
      "         [-0.0411,  0.3431,  0.2645],\n",
      "         [-0.0411,  0.3431,  0.2645],\n",
      "         [ 1.1864,  0.5637, -1.1194]],\n",
      "\n",
      "        [[ 0.8141, -0.1185, -0.0485],\n",
      "         [ 0.8141, -0.1185, -0.0485],\n",
      "         [ 1.1864,  0.5637, -1.1194],\n",
      "         [ 0.3134,  0.5307, -0.2227],\n",
      "         [ 0.3134,  0.5307, -0.2227]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(51,3)\n",
    "input = torch.LongTensor([[9,5,9,1,2],[4,1,2,2,2],[40,40,40,40,1],[4,4,1,2,2]])\n",
    "embedded=embedding(input)\n",
    "print(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([[-0.0411,  0.3431,  0.2645],\n",
      "        [ 1.4367, -1.2376, -0.7855],\n",
      "        [ 0.8141, -0.1185, -0.0485],\n",
      "        [ 0.8141, -0.1185, -0.0485],\n",
      "        [-0.0411,  0.3431,  0.2645],\n",
      "        [ 0.1500,  0.1474, -0.2132],\n",
      "        [ 0.8141, -0.1185, -0.0485],\n",
      "        [ 1.1864,  0.5637, -1.1194],\n",
      "        [-0.0411,  0.3431,  0.2645],\n",
      "        [ 1.4367, -1.2376, -0.7855],\n",
      "        [ 1.1864,  0.5637, -1.1194],\n",
      "        [-0.0411,  0.3431,  0.2645],\n",
      "        [ 1.1864,  0.5637, -1.1194],\n",
      "        [ 1.1864,  0.5637, -1.1194]], grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([4, 4, 3, 2, 1]), sorted_indices=tensor([2, 0, 3, 1]), unsorted_indices=tensor([1, 3, 0, 2]))\n"
     ]
    }
   ],
   "source": [
    "leng = torch.tensor([4,2,5,3])\n",
    "packed = pack_padded_sequence(embedded, leng, batch_first=True, enforce_sorted=False)\n",
    "#print('\\n\\n')\n",
    "print(packed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](https://yifdu.github.io/2019/03/28/Pytorch-tutorials-%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89/pic4.png \"Optional title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2598, -0.0980, -1.2113],\n",
       "        [-2.5211,  0.1555,  0.3407],\n",
       "        [ 0.4294, -0.0681,  0.2158]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(3,3) #len=3, batch=2, char =5 kinds\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5184, 0.3625, 0.1191],\n",
       "        [0.0303, 0.4401, 0.5296],\n",
       "        [0.4139, 0.2517, 0.3344]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = nn.Softmax(dim=1)\n",
    "# dim means Softmax's dim, here use 2 (char kind)\n",
    "sm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6569, -1.0147, -2.1281],\n",
       "        [-3.4973, -0.8208, -0.6356],\n",
       "        [-0.8820, -1.3795, -1.0956]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(sm(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLLLoss的結果就是把上面的輸出與Label對應的那個值拿出來，再去掉負號，再求均值\n",
    "ex : ground truth是(0,2,1)，那NLLLose的算法是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8906666666666666"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.6569+0.6356+1.3795)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8907)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss=nn.NLLLoss()\n",
    "target=torch.tensor([0,2,1])\n",
    "loss(torch.log(sm(input)),target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch (3 dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2760,  0.4598,  0.2189,  0.4483,  0.9722],\n",
       "         [ 0.8616, -1.2340,  1.1155,  1.8708,  0.5987]],\n",
       "\n",
       "        [[-1.0815, -0.0203, -0.1701, -0.8232,  1.5321],\n",
       "         [ 1.6136,  0.6235, -0.9264, -0.0197,  1.6300]],\n",
       "\n",
       "        [[ 0.6959,  0.1608,  0.4912,  0.4067,  0.4182],\n",
       "         [ 0.6458, -0.0346, -0.4760, -0.9156, -0.3314]]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(3,2,5)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.3297, -1.5939, -1.8348, -1.6054, -1.0815],\n",
       "         [-1.7790, -3.8746, -1.5252, -0.7699, -2.0419]],\n",
       "\n",
       "        [[-3.0597, -1.9984, -2.1482, -2.8013, -0.4461],\n",
       "         [-0.9792, -1.9693, -3.5192, -2.6124, -0.9628]],\n",
       "\n",
       "        [[-1.3627, -1.8978, -1.5675, -1.6520, -1.6404],\n",
       "         [-0.8831, -1.5634, -2.0049, -2.4445, -1.8602]]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(sm(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2062166666666667"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((2.3297+2.1482+1.8978)/3 + (1.7790+3.5192+1.5634)/3)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2062)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss=nn.NLLLoss()\n",
    "\n",
    "target=torch.tensor([[0,0],[2,2,],[1,1]])\n",
    "targets = target.contiguous().view(-1)  # S = (B*T)       #to one dim\n",
    "decoder_outputs = torch.log(sm(input)).view(3*2, -1)  # S = (B*T) x V   #b*t,30   #counting cost, only require pairs to be meeted\n",
    "\n",
    "loss(decoder_outputs,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
