{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab information:\n",
      "Char: SOS Index: 0\n",
      "Char: EOS Index: 1\n",
      "Char: PAD Index: 2\n",
      "Char: UNK Index: 3\n",
      "Char: t Index: 4\n",
      "Char: h Index: 5\n",
      "Char: e Index: 6\n",
      "Char: o Index: 7\n",
      "Char: f Index: 8\n",
      "Char: a Index: 9\n",
      "Char: n Index: 10\n",
      "Char: d Index: 11\n",
      "Char: i Index: 12\n",
      "Char: r Index: 13\n",
      "Char: s Index: 14\n",
      "Char: b Index: 15\n",
      "Char: y Index: 16\n",
      "Char: w Index: 17\n",
      "Char: u Index: 18\n",
      "Char: m Index: 19\n",
      "Char: l Index: 20\n",
      "Char: v Index: 21\n",
      "Char: c Index: 22\n",
      "Char: p Index: 23\n",
      "Char: g Index: 24\n",
      "Char: k Index: 25\n",
      "Char: x Index: 26\n",
      "Char: j Index: 27\n",
      "Char: z Index: 28\n",
      "Char: q Index: 29\n",
      "\n",
      "Sequence before transformed: helloworld\n",
      "Indices sequence: [5, 6, 20, 20, 7, 17, 7, 13, 20, 11]\n",
      "Sequence after transformed: helloworld \n",
      "\n",
      "\n",
      "mini:\n",
      "[[[[27, 7, 15, 14, 1], [27, 7, 15, 14, 1]], [[10, 6, 4, 14, 22, 9, 23, 6, 1], [10, 6, 4, 14, 22, 9, 23, 6, 1]], [[9, 20, 7, 10, 24, 1], [9, 20, 7, 10, 24, 1]]], [[[17, 6, 11, 1], [17, 6, 11, 1]], [[24, 7, 21, 6, 13, 10, 19, 6, 10, 4, 14, 1], [24, 7, 21, 6, 13, 10, 19, 6, 10, 4, 14, 1]], [[7, 23, 23, 7, 10, 6, 10, 4, 1], [7, 23, 23, 7, 10, 6, 10, 4, 1]]]]\n",
      "\n",
      "\n",
      "B0-0\n",
      "(tensor([[10,  9, 27],\n",
      "        [ 6, 20,  7],\n",
      "        [ 4,  7, 15],\n",
      "        [14, 10, 14],\n",
      "        [22, 24,  1],\n",
      "        [ 9,  1,  2],\n",
      "        [23,  2,  2],\n",
      "        [ 6,  2,  2],\n",
      "        [ 1,  2,  2]]), [9, 6, 5])\n",
      "------------\n",
      "tensor([[[ 0.3811,  0.7674,  0.6963],\n",
      "         [ 0.2410,  1.0168, -1.1578],\n",
      "         [ 0.7322, -0.9060,  0.1610]],\n",
      "\n",
      "        [[-2.4704, -0.4086,  0.2229],\n",
      "         [ 1.0763,  1.1575, -0.1620],\n",
      "         [-0.0942,  0.8659, -1.4522]],\n",
      "\n",
      "        [[ 0.1728, -0.0712, -3.1212],\n",
      "         [-0.0942,  0.8659, -1.4522],\n",
      "         [-0.1633, -0.7445,  0.8409]],\n",
      "\n",
      "        [[-0.4533, -0.4538,  0.1005],\n",
      "         [ 0.3811,  0.7674,  0.6963],\n",
      "         [-0.4533, -0.4538,  0.1005]],\n",
      "\n",
      "        [[ 0.1241,  0.5611,  0.9111],\n",
      "         [ 0.6712,  1.4709,  0.3834],\n",
      "         [-0.7736,  0.1940,  0.7497]],\n",
      "\n",
      "        [[ 0.2410,  1.0168, -1.1578],\n",
      "         [-0.7736,  0.1940,  0.7497],\n",
      "         [-0.4129, -0.4232, -0.4672]],\n",
      "\n",
      "        [[-1.3813,  2.0789, -0.2463],\n",
      "         [-0.4129, -0.4232, -0.4672],\n",
      "         [-0.4129, -0.4232, -0.4672]],\n",
      "\n",
      "        [[-2.4704, -0.4086,  0.2229],\n",
      "         [-0.4129, -0.4232, -0.4672],\n",
      "         [-0.4129, -0.4232, -0.4672]],\n",
      "\n",
      "        [[-0.7736,  0.1940,  0.7497],\n",
      "         [-0.4129, -0.4232, -0.4672],\n",
      "         [-0.4129, -0.4232, -0.4672]]], grad_fn=<EmbeddingBackward>)\n",
      "============================================\n",
      "<class 'list'>\n",
      "[9, 6, 5]\n",
      "\n",
      "\n",
      "\n",
      "PackedSequence(data=tensor([[ 0.3811,  0.7674,  0.6963],\n",
      "        [ 0.2410,  1.0168, -1.1578],\n",
      "        [ 0.7322, -0.9060,  0.1610],\n",
      "        [-2.4704, -0.4086,  0.2229],\n",
      "        [ 1.0763,  1.1575, -0.1620],\n",
      "        [-0.0942,  0.8659, -1.4522],\n",
      "        [ 0.1728, -0.0712, -3.1212],\n",
      "        [-0.0942,  0.8659, -1.4522],\n",
      "        [-0.1633, -0.7445,  0.8409],\n",
      "        [-0.4533, -0.4538,  0.1005],\n",
      "        [ 0.3811,  0.7674,  0.6963],\n",
      "        [-0.4533, -0.4538,  0.1005],\n",
      "        [ 0.1241,  0.5611,  0.9111],\n",
      "        [ 0.6712,  1.4709,  0.3834],\n",
      "        [-0.7736,  0.1940,  0.7497],\n",
      "        [ 0.2410,  1.0168, -1.1578],\n",
      "        [-0.7736,  0.1940,  0.7497],\n",
      "        [-1.3813,  2.0789, -0.2463],\n",
      "        [-2.4704, -0.4086,  0.2229],\n",
      "        [-0.7736,  0.1940,  0.7497]], grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([3, 3, 3, 3, 3, 2, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.char2idx = {'SOS': 0, 'EOS': 1, 'PAD': 2, 'UNK': 3}\n",
    "        self.idx2char = {0: 'SOS', 1: 'EOS', 2: 'PAD', 3: 'UNK'}\n",
    "        self.num_chars = 4\n",
    "        self.max_length = 0\n",
    "        self.word_list = []\n",
    "\n",
    "    def build_vocab(self, data_path):\n",
    "        \"\"\"Construct the relation between words and indices\"\"\"\n",
    "        with open(data_path, 'r', encoding='utf-8') as dataset:\n",
    "            for word in dataset:\n",
    "                word = word.strip('\\n')\n",
    "\n",
    "                self.word_list.append(word)\n",
    "                if self.max_length < len(word):\n",
    "                    self.max_length = len(word)\n",
    "\n",
    "                chars = self.split_sequence(word)\n",
    "                for char in chars:\n",
    "                    if char not in self.char2idx:\n",
    "                        self.char2idx[char] = self.num_chars\n",
    "                        self.idx2char[self.num_chars] = char\n",
    "                        self.num_chars += 1\n",
    "\n",
    "    def sequence_to_indices(self, sequence, add_eos=False, add_sos=False):\n",
    "        \"\"\"Transform a char sequence to index sequence\n",
    "            :param sequence: a string composed with chars\n",
    "            :param add_eos: if true, add the <EOS> tag at the end of given sentence\n",
    "            :param add_sos: if true, add the <SOS> tag at the beginning of given sentence\n",
    "        \"\"\"\n",
    "        index_sequence = [self.char2idx['SOS']] if add_sos else []\n",
    "\n",
    "        for char in self.split_sequence(sequence):\n",
    "            if char not in self.char2idx:\n",
    "                index_sequence.append((self.char2idx['UNK']))\n",
    "            else:\n",
    "                index_sequence.append(self.char2idx[char])\n",
    "\n",
    "        if add_eos:\n",
    "            index_sequence.append(self.char2idx['EOS'])\n",
    "\n",
    "        return index_sequence\n",
    "\n",
    "    def indices_to_sequence(self, indices):\n",
    "        \"\"\"Transform a list of indices\n",
    "            :param indices: a list\n",
    "        \"\"\"\n",
    "        sequence = \"\"\n",
    "        for idx in indices:\n",
    "            char = self.idx2char[idx]\n",
    "            if char == \"EOS\":\n",
    "                break\n",
    "            else:\n",
    "                sequence += char\n",
    "        return sequence\n",
    "\n",
    "    def split_sequence(self, sequence):\n",
    "        \"\"\"Vary from languages and tasks. In our task, we simply return chars in given sentence\n",
    "        For example:\n",
    "            Input : alphabet\n",
    "            Return: [a, l, p, h, a, b, e, t]\n",
    "        \"\"\"\n",
    "        return [char for char in sequence]\n",
    "\n",
    "    def __str__(self):\n",
    "        str = \"Vocab information:\\n\"\n",
    "        for idx, char in self.idx2char.items():\n",
    "            str += \"Char: %s Index: %d\\n\" % (char, idx)\n",
    "        return str\n",
    "\n",
    "\n",
    "class DataTransformer(object):\n",
    "\n",
    "    def __init__(self, path, use_cuda):\n",
    "        self.indices_sequences = []\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # Load and build the vocab\n",
    "        self.vocab = Vocabulary()\n",
    "        self.vocab.build_vocab(path)\n",
    "        self.PAD_ID = self.vocab.char2idx[\"PAD\"]\n",
    "        self.SOS_ID = self.vocab.char2idx[\"SOS\"]\n",
    "        self.vocab_size = self.vocab.num_chars\n",
    "        self.max_length = self.vocab.max_length\n",
    "\n",
    "        self._build_training_set(path)\n",
    "\n",
    "    def _build_training_set(self, path):\n",
    "        # Change sentences to indices, and append <EOS> at the end of all pairs\n",
    "        for word in self.vocab.word_list:\n",
    "            indices_seq = self.vocab.sequence_to_indices(word, add_eos=True)\n",
    "            # input and target are the same in auto-encoder\n",
    "            self.indices_sequences.append([indices_seq, indices_seq[:]])\n",
    "            if indices_seq==indices_seq[:]:\n",
    "                print([indices_seq, indices_seq[:]])\n",
    "\n",
    "    def mini_batches(self, batch_size):\n",
    "        input_batches = []\n",
    "        target_batches = []\n",
    "\n",
    "        np.random.shuffle(self.indices_sequences)\n",
    "\n",
    "        #print(self.indices_sequences[0:0+batch_size])\n",
    "        mini_batches = [\n",
    "            self.indices_sequences[k: k + batch_size]\n",
    "            for k in range(0, len(self.indices_sequences), batch_size)\n",
    "        ]      \n",
    "        print('\\nmini:')\n",
    "        print(mini_batches[0:2])\n",
    "\n",
    "        for batch in mini_batches:\n",
    "            seq_pairs = sorted(batch, key=lambda seqs: len(seqs[0]), reverse=True)  # sorted by input_lengths\n",
    "            input_seqs = [pair[0] for pair in seq_pairs]\n",
    "            target_seqs = [pair[1] for pair in seq_pairs]\n",
    "\n",
    "            input_lengths = [len(s) for s in input_seqs]\n",
    "            in_max = input_lengths[0]\n",
    "            input_padded = [self.pad_sequence(s, in_max) for s in input_seqs]\n",
    "\n",
    "            target_lengths = [len(s) for s in target_seqs]\n",
    "            out_max = target_lengths[0]\n",
    "            target_padded = [self.pad_sequence(s, out_max) for s in target_seqs]\n",
    "\n",
    "            input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)  # time * batch\n",
    "            target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)  # time * batch\n",
    "\n",
    "            if self.use_cuda:\n",
    "                input_var = input_var.cuda()\n",
    "                target_var = target_var.cuda()\n",
    "\n",
    "            yield (input_var, input_lengths), (target_var, target_lengths)\n",
    "\n",
    "    def pad_sequence(self, sequence, max_length):\n",
    "        sequence += [self.PAD_ID for i in range(max_length - len(sequence))]\n",
    "        return sequence\n",
    "\n",
    "    def evaluation_batch(self, words):\n",
    "        \"\"\"\n",
    "        Prepare a batch of var for evaluating\n",
    "        :param words: a list, store the testing data \n",
    "        :return: evaluation_batch\n",
    "        \"\"\"\n",
    "        evaluation_batch = []\n",
    "\n",
    "        for word in words:\n",
    "            indices_seq = self.vocab.sequence_to_indices(word, add_eos=True)\n",
    "            evaluation_batch.append([indices_seq])\n",
    "\n",
    "        seq_pairs = sorted(evaluation_batch, key=lambda seqs: len(seqs[0]), reverse=True)\n",
    "        input_seqs = [pair[0] for pair in seq_pairs]\n",
    "        input_lengths = [len(s) for s in input_seqs]\n",
    "        in_max = input_lengths[0]\n",
    "        input_padded = [self.pad_sequence(s, in_max) for s in input_seqs]\n",
    "\n",
    "        input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)  # time * batch\n",
    "\n",
    "        if self.use_cuda:\n",
    "            input_var = input_var.cuda()\n",
    "\n",
    "        return input_var, input_lengths\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    vocab = Vocabulary()\n",
    "    vocab.build_vocab('Google-10000-English.txt')\n",
    "    print(vocab)\n",
    "\n",
    "    test = \"helloworld\"\n",
    "    print(\"Sequence before transformed:\", test)\n",
    "    ids = vocab.sequence_to_indices(test)\n",
    "    print(\"Indices sequence:\", ids)\n",
    "    sent = vocab.indices_to_sequence(ids)\n",
    "    print(\"Sequence after transformed:\",sent,\"\\n\")\n",
    "\n",
    "    data_transformer = DataTransformer('Google-10000-English.txt', use_cuda=False)\n",
    "\n",
    "    for ib, tb in data_transformer.mini_batches(batch_size=3):\n",
    "        print(\"\\n\\nB0-0\")\n",
    "        print(ib)#,'\\n\\n', tb)\n",
    "        \n",
    "        print('------------')\n",
    "        embedding = nn.Embedding(30,3)\n",
    "        input=ib[0]\n",
    "        embedded=embedding(input)\n",
    "        print(embedded)\n",
    "        \n",
    "        print('============================================')\n",
    "        leng = ib[1]\n",
    "        print(type(leng))\n",
    "        print(leng)\n",
    "        packed = pack_padded_sequence(embedded, leng)\n",
    "        print('\\n\\n')\n",
    "        print(packed)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "embedding = nn.Embedding(51,3)\n",
    "input = torch.LongTensor([[9,5,9,1,2],[4,1,2,2,2],[40,40,40,40,1],[4,4,1,2,2]])\n",
    "embedded=embedding(input)\n",
    "print(embedded)\n",
    "\n",
    "\n",
    "leng = torch.tensor([4,2,5,3])\n",
    "packed = pack_padded_sequence(embedded, leng, batch_first=True, enforce_sorted=False)\n",
    "print('\\n\\n')\n",
    "print(packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mini:\n",
      "[[[[23, 5, 9, 10, 4, 7, 19, 1], [23, 5, 9, 10, 4, 7, 19, 1]], [[24, 9, 15, 13, 12, 6, 20, 1], [24, 9, 15, 13, 12, 6, 20, 1]], [[14, 6, 6, 11, 1], [14, 6, 6, 11, 1]]], [[[22, 7, 13, 13, 18, 23, 4, 12, 7, 10, 1], [22, 7, 13, 13, 18, 23, 4, 12, 7, 10, 1]], [[6, 23, 12, 14, 7, 11, 6, 14, 1], [6, 23, 12, 14, 7, 11, 6, 14, 1]], [[6, 26, 23, 20, 7, 13, 6, 1], [6, 26, 23, 20, 7, 13, 6, 1]]]]\n",
      "tensor([[[-0.5840,  0.2594, -0.3392],\n",
      "         [ 0.9600, -0.8475, -0.9238],\n",
      "         [-0.3586,  1.8042, -0.0465]],\n",
      "\n",
      "        [[-0.1190,  0.7071, -0.5437],\n",
      "         [-0.1076, -1.4298, -0.8118],\n",
      "         [-0.6915, -1.4748, -0.8687]],\n",
      "\n",
      "        [[-0.1076, -1.4298, -0.8118],\n",
      "         [ 0.8873,  0.4930,  1.2483],\n",
      "         [-0.6915, -1.4748, -0.8687]],\n",
      "\n",
      "        [[-0.5564,  0.9398,  0.5524],\n",
      "         [-0.9800, -0.2611,  0.5359],\n",
      "         [ 0.0237,  0.5845, -0.3780]],\n",
      "\n",
      "        [[ 1.5539, -0.4662,  1.2247],\n",
      "         [-0.1590, -2.0524,  0.5597],\n",
      "         [ 0.3480,  0.8226,  0.3421]],\n",
      "\n",
      "        [[-0.9431,  0.3944,  0.6962],\n",
      "         [-0.6915, -1.4748, -0.8687],\n",
      "         [-0.1267, -0.4115,  0.5815]],\n",
      "\n",
      "        [[-0.3547,  1.0052,  1.1554],\n",
      "         [ 1.1602, -0.2683, -0.5722],\n",
      "         [-0.1267, -0.4115,  0.5815]],\n",
      "\n",
      "        [[ 0.3480,  0.8226,  0.3421],\n",
      "         [ 0.3480,  0.8226,  0.3421],\n",
      "         [-0.1267, -0.4115,  0.5815]]], grad_fn=<EmbeddingBackward>)\n",
      "\n",
      "\n",
      "\n",
      "PackedSequence(data=tensor([[-0.5840,  0.2594, -0.3392],\n",
      "        [ 0.9600, -0.8475, -0.9238],\n",
      "        [-0.3586,  1.8042, -0.0465],\n",
      "        [-0.1190,  0.7071, -0.5437],\n",
      "        [-0.1076, -1.4298, -0.8118],\n",
      "        [-0.6915, -1.4748, -0.8687],\n",
      "        [-0.1076, -1.4298, -0.8118],\n",
      "        [ 0.8873,  0.4930,  1.2483],\n",
      "        [-0.6915, -1.4748, -0.8687],\n",
      "        [-0.5564,  0.9398,  0.5524],\n",
      "        [-0.9800, -0.2611,  0.5359],\n",
      "        [ 0.0237,  0.5845, -0.3780],\n",
      "        [ 1.5539, -0.4662,  1.2247],\n",
      "        [-0.1590, -2.0524,  0.5597],\n",
      "        [ 0.3480,  0.8226,  0.3421],\n",
      "        [-0.9431,  0.3944,  0.6962],\n",
      "        [-0.6915, -1.4748, -0.8687],\n",
      "        [-0.3547,  1.0052,  1.1554],\n",
      "        [ 1.1602, -0.2683, -0.5722],\n",
      "        [ 0.3480,  0.8226,  0.3421],\n",
      "        [ 0.3480,  0.8226,  0.3421]], grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([3, 3, 3, 3, 3, 2, 2, 2]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "for ib, tb in data_transformer.mini_batches(batch_size=3):\n",
    "        embedding = nn.Embedding(30,3)\n",
    "        input=ib[0]\n",
    "        embedded=embedding(input)\n",
    "        print(embedded)\n",
    "        \n",
    "        leng = ib[1]\n",
    "        #print(type(leng))\n",
    "        #print(leng)\n",
    "        packed = pack_padded_sequence(embedded, leng)\n",
    "        print('\\n\\n')\n",
    "        print(packed)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[[[[1, 2, 3, 4], [1, 2, 3, 4]], [[4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9]], [[7, 8, 9], [7, 8, 9]]], [[[10, 11], [10, 11]], [[1, 3], [1, 3]], [[44, 45], [44, 45]]]]\n",
      "----------------------------------\n",
      "[[[1, 2, 3, 4], [1, 2, 3, 4]], [[4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9]], [[7, 8, 9], [7, 8, 9]]]\n",
      "[[[4, 5, 6, 7, 8, 9], [4, 5, 6, 7, 8, 9]], [[1, 2, 3, 4], [1, 2, 3, 4]], [[7, 8, 9], [7, 8, 9]]]\n",
      "[[4, 5, 6, 7, 8, 9], [1, 2, 3, 4], [7, 8, 9]]\n",
      "[[4, 5, 6, 7, 8, 9], [1, 2, 3, 4], [7, 8, 9]]\n",
      "\n",
      "\n",
      "\n",
      "[[[10, 11], [10, 11]], [[1, 3], [1, 3]], [[44, 45], [44, 45]]]\n",
      "[[[10, 11], [10, 11]], [[1, 3], [1, 3]], [[44, 45], [44, 45]]]\n",
      "[[10, 11], [1, 3], [44, 45]]\n",
      "[[10, 11], [1, 3], [44, 45]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = [[[1,2,3,4],[1,2,3,4]],[[4,5,6,7,8,9],[4,5,6,7,8,9]],[[7,8,9],[7,8,9]],[[10,11],[10,11]], [[1,3],[1,3]],[[44,45],[44,45]]]\n",
    "batch_size=3\n",
    "print(len(a))\n",
    "b = [\n",
    "        a[k: k + batch_size]\n",
    "        for k in range(0, len(a), batch_size)\n",
    "    ]\n",
    "print(b)\n",
    "print('----------------------------------')\n",
    "for batch in b:\n",
    "    print(batch)\n",
    "    seq_pairs = sorted(batch, key=lambda seqs: len(seqs[0]), reverse=True)  # sorted by input_lengths\n",
    "    print(seq_pairs)\n",
    "    input_seqs = [pair[0] for pair in seq_pairs]\n",
    "    print(input_seqs)\n",
    "    target_seqs = [pair[1] for pair in seq_pairs]\n",
    "    print(target_seqs)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-75-71764765c2cc>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-75-71764765c2cc>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    decoder_input = Variable(torch.LongTensor([[2 * 128])))\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "decoder_input = Variable(torch.LongTensor([[2 * 128])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "decoder_input = Variable(torch.LongTensor([[1] * 128]))\n",
    "print(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "decoder_outputs = Variable(torch.zeros(\n",
    "            8,\n",
    "            3,        #128\n",
    "            4  #30\n",
    "        ))\n",
    "print(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
