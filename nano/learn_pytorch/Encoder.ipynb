{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class VanillaEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, output_size):\n",
    "        \"\"\"Define layers for a vanilla rnn encoder\"\"\"\n",
    "        super(VanillaEncoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, output_size)\n",
    "\n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        packed = pack_padded_sequence(embedded, input_lengths)\n",
    "        packed_outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = pad_packed_sequence(packed_outputs)\n",
    "        return outputs, hidden\n",
    "\n",
    "    def forward_a_sentence(self, inputs, hidden=None):\n",
    "        \"\"\"Deprecated, forward 'one' sentence at a time which is bad for gpu utilization\"\"\"\n",
    "        embedded = self.embedding(inputs)\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, output_size, max_length, teacher_forcing_ratio, sos_id, use_cuda):\n",
    "        \"\"\"Define layers for a vanilla rnn decoder\"\"\"\n",
    "        super(VanillaDecoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.log_softmax = nn.LogSoftmax()  # work with NLLLoss = CrossEntropyLoss\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.sos_id = sos_id\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "    def forward_step(self, inputs, hidden):\n",
    "        # inputs: (time_steps=1, batch_size)\n",
    "        batch_size = inputs.size(1)\n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded.view(1, batch_size, self.hidden_size)  # S = T(1) x B x N\n",
    "        rnn_output, hidden = self.gru(embedded, hidden)  # S = T(1) x B x H\n",
    "        rnn_output = rnn_output.squeeze(0)  # squeeze the time dimension\n",
    "        output = self.log_softmax(self.out(rnn_output))  # S = B x O\n",
    "        return output, hidden\n",
    "\n",
    "    def forward(self, context_vector, targets):\n",
    "\n",
    "        # Prepare variable for decoder on time_step_0\n",
    "        target_vars, target_lengths = targets\n",
    "        batch_size = context_vector.size(1)\n",
    "        decoder_input = Variable(torch.LongTensor([[self.sos_id] * batch_size]))\n",
    "\n",
    "        # Pass the context vector\n",
    "        decoder_hidden = context_vector\n",
    "\n",
    "        max_target_length = max(target_lengths)\n",
    "        decoder_outputs = Variable(torch.zeros(\n",
    "            max_target_length,\n",
    "            batch_size,\n",
    "            self.output_size\n",
    "        ))  # (time_steps, batch_size, vocab_size)\n",
    "\n",
    "        if self.use_cuda:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "            decoder_outputs = decoder_outputs.cuda()\n",
    "\n",
    "        use_teacher_forcing = True if random.random() > self.teacher_forcing_ratio else False\n",
    "\n",
    "        # Unfold the decoder RNN on the time dimension\n",
    "        for t in range(max_target_length):\n",
    "            decoder_outputs_on_t, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs[t] = decoder_outputs_on_t\n",
    "            if use_teacher_forcing:\n",
    "                decoder_input = target_vars[t].unsqueeze(0)\n",
    "            else:\n",
    "                decoder_input = self._decode_to_index(decoder_outputs_on_t)\n",
    "            return decoder_outputs, decoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        input_vars, input_lengths = inputs\n",
    "        encoder_outputs, encoder_hidden = self.encoder.forward(input_vars, input_lengths)\n",
    "        decoder_outputs, decoder_hidden = self.decoder.forward(context_vector=encoder_hidden, targets=targets)\n",
    "        return decoder_outputs, decoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'config' has no attribute 'checkpoint_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-560369f23a8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     def __init__(self, model, data_transformer, learning_rate, use_cuda,\n\u001b[1;32m      5\u001b[0m                  \u001b[0mcheckpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-560369f23a8a>\u001b[0m in \u001b[0;36mTrainer\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     def __init__(self, model, data_transformer, learning_rate, use_cuda,\n\u001b[0;32m----> 5\u001b[0;31m                  \u001b[0mcheckpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                  teacher_forcing_ratio=config.teacher_forcing_ratio):\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'config' has no attribute 'checkpoint_name'"
     ]
    }
   ],
   "source": [
    "import config\n",
    "class Trainer(object):\n",
    "\n",
    "    def __init__(self, model, data_transformer, learning_rate, use_cuda,\n",
    "                 checkpoint_name=config.checkpoint_name,\n",
    "                 teacher_forcing_ratio=config.teacher_forcing_ratio):\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        # record some information about dataset\n",
    "        self.data_transformer = data_transformer\n",
    "        self.vocab_size = self.data_transformer.vocab_size\n",
    "        self.PAD_ID = self.data_transformer.PAD_ID\n",
    "        self.use_cuda = use_cuda\n",
    "\n",
    "        # optimizer setting\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer= torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.criterion = torch.nn.NLLLoss(ignore_index=self.PAD_ID, size_average=True)\n",
    "\n",
    "        self.checkpoint_name = checkpoint_name\n",
    "\n",
    "    def train(self, num_epochs, batch_size, pretrained=False):\n",
    "\n",
    "        if pretrained:\n",
    "            self.load_model()\n",
    "\n",
    "        for epoch in range(0, num_epochs):\n",
    "            mini_batches = self.data_transformer.mini_batches(batch_size=batch_size)\n",
    "            for input_batch, target_batch in mini_batches:\n",
    "                self.optimizer.zero_grad()\n",
    "                decoder_outputs, decoder_hidden = self.model(input_batch, target_batch)\n",
    "                # calculate the loss and back prop.\n",
    "                cur_loss = self.get_loss(decoder_outputs, target_batch[0])\n",
    "                cur_loss.backward()\n",
    "                # optimize\n",
    "                self.optimizer.step()\n",
    "\n",
    "        self.save_model()\n",
    "\n",
    "    def get_loss(self, decoder_outputs, targets):\n",
    "        b = decoder_outputs.size(1)\n",
    "        t = decoder_outputs.size(0)\n",
    "        targets = targets.contiguous().view(-1)  # S = (B*T)\n",
    "        decoder_outputs = decoder_outputs.view(b * t, -1)  # S = (B*T) x V\n",
    "        return self.criterion(decoder_outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0077,  0.3788, -1.3240],\n",
      "         [-0.6990, -0.7185,  0.5279],\n",
      "         [ 1.0077,  0.3788, -1.3240],\n",
      "         [ 0.8759,  0.2679, -1.5691],\n",
      "         [-0.0909,  0.3856,  1.0976]],\n",
      "\n",
      "        [[-0.0590, -0.6038,  0.7650],\n",
      "         [ 0.8759,  0.2679, -1.5691],\n",
      "         [-0.0909,  0.3856,  1.0976],\n",
      "         [-0.0909,  0.3856,  1.0976],\n",
      "         [-0.0909,  0.3856,  1.0976]],\n",
      "\n",
      "        [[ 0.0566,  1.4179,  0.0086],\n",
      "         [ 0.0566,  1.4179,  0.0086],\n",
      "         [ 0.0566,  1.4179,  0.0086],\n",
      "         [ 0.0566,  1.4179,  0.0086],\n",
      "         [ 0.8759,  0.2679, -1.5691]],\n",
      "\n",
      "        [[-0.0590, -0.6038,  0.7650],\n",
      "         [-0.0590, -0.6038,  0.7650],\n",
      "         [ 0.8759,  0.2679, -1.5691],\n",
      "         [-0.0909,  0.3856,  1.0976],\n",
      "         [-0.0909,  0.3856,  1.0976]]], grad_fn=<EmbeddingBackward>)\n",
      "\n",
      "\n",
      "\n",
      "PackedSequence(data=tensor([[ 0.0566,  1.4179,  0.0086],\n",
      "        [ 1.0077,  0.3788, -1.3240],\n",
      "        [-0.0590, -0.6038,  0.7650],\n",
      "        [-0.0590, -0.6038,  0.7650],\n",
      "        [ 0.0566,  1.4179,  0.0086],\n",
      "        [-0.6990, -0.7185,  0.5279],\n",
      "        [-0.0590, -0.6038,  0.7650],\n",
      "        [ 0.8759,  0.2679, -1.5691],\n",
      "        [ 0.0566,  1.4179,  0.0086],\n",
      "        [ 1.0077,  0.3788, -1.3240],\n",
      "        [ 0.8759,  0.2679, -1.5691],\n",
      "        [ 0.0566,  1.4179,  0.0086],\n",
      "        [ 0.8759,  0.2679, -1.5691],\n",
      "        [ 0.8759,  0.2679, -1.5691]], grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([4, 4, 3, 2, 1]), sorted_indices=tensor([2, 0, 3, 1]), unsorted_indices=tensor([1, 3, 0, 2]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "embedding = nn.Embedding(51,3)\n",
    "input = torch.LongTensor([[9,5,9,1,2],[4,1,2,2,2],[40,40,40,40,1],[4,4,1,2,2]])\n",
    "embedded=embedding(input)\n",
    "print(embedded)\n",
    "\n",
    "\n",
    "leng = torch.tensor([4,2,5,3])\n",
    "packed = pack_padded_sequence(embedded, leng, batch_first=True, enforce_sorted=False)\n",
    "print('\\n\\n')\n",
    "print(packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([[-8.8804e-03, -2.2964e-01, -1.0186e-01],\n",
      "        [-5.2256e-01,  3.4638e-01,  6.6066e-02],\n",
      "        [-2.0985e-01, -3.9370e-01, -7.8117e-03],\n",
      "        [-2.0985e-01, -3.9370e-01, -7.8117e-03],\n",
      "        [-2.5305e-02, -3.2686e-01, -1.7162e-01],\n",
      "        [-2.6614e-01, -4.6779e-01,  5.1146e-02],\n",
      "        [-3.5545e-01, -4.8351e-01,  2.8765e-04],\n",
      "        [-9.9764e-02, -7.7682e-01,  3.4367e-01],\n",
      "        [-3.6792e-02, -3.6453e-01, -2.1622e-01],\n",
      "        [-6.2072e-01,  2.0237e-01,  1.2637e-01],\n",
      "        [-2.3836e-01, -7.9580e-01,  3.6030e-01],\n",
      "        [-4.2640e-02, -3.7810e-01, -2.4370e-01],\n",
      "        [-4.1420e-01, -6.4108e-01,  4.0831e-01],\n",
      "        [ 5.6516e-02, -7.7115e-01,  2.6235e-01]], grad_fn=<CatBackward>), batch_sizes=tensor([4, 4, 3, 2, 1]), sorted_indices=tensor([2, 0, 3, 1]), unsorted_indices=tensor([1, 3, 0, 2]))\n",
      "---------------\n",
      "tensor([[[-0.4142, -0.6411,  0.4083],\n",
      "         [-0.0998, -0.7768,  0.3437],\n",
      "         [ 0.0565, -0.7712,  0.2624],\n",
      "         [-0.2384, -0.7958,  0.3603]]], grad_fn=<IndexSelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "packed_outputs, hidden=gru(packed, None)\n",
    "print(packed_outputs)\n",
    "print('---------------')\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
